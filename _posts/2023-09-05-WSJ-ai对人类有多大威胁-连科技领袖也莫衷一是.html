---
layout: post
title: AI对人类有多大威胁？连科技领袖也莫衷一是
date: 2023-09-05 12:33:03.000000000 +08:00
link: https://cn.wsj.com/amp/articles/ai%E5%AF%B9%E4%BA%BA%E7%B1%BB%E6%9C%89%E5%A4%9A%E5%A4%A7%E5%A8%81%E8%83%81-%E8%BF%9E%E7%A7%91%E6%8A%80%E9%A2%86%E8%A2%96%E4%B9%9F%E8%8E%AB%E8%A1%B7%E4%B8%80%E6%98%AF-2e70b66e
categories: wsj
---

<main id="main" role="main">
<div>


</div>
<div itemprop="articleLead" data-sbId="CN-TEC-20230905113519">
    <div>
      <div class="media-object scope-
          header
">
          <figure>
      <div>
        <img
          srcset="https://images.wsj.net/im-846244?width=540&amp;size=1 540w, https://images.wsj.net/im-846244?width=620&amp;size=1 620w, https://images.wsj.net/im-846244?width=639&amp;size=1 639w, https://images.wsj.net/im-846244?width=860&amp;size=1 860w, https://images.wsj.net/im-846244?width=860&amp;size=1&amp;pixel_ratio=1.5 1290w, https://images.wsj.net/im-846244?width=860&amp;size=1&amp;pixel_ratio=2 1720w, https://images.wsj.net/im-846244?width=860&amp;size=1&amp;pixel_ratio=3 2580w"
          src="https://images.wsj.net/im-846244?width=860&amp;height=860"
          layout="responsive"
          placeholder
          alt="">
        </img>
      </div>
      <figcaption>
        <p> 图片来源：Stephanie Aaronson/The Wall Street Journal; Photos: iStock</p>
  </figcaption>
</figure>

          <!-- eventually when we know what this card will be we can change it and leave this one -->
      </div>
    </div>
</div>
<div data-sbId="CN-TEC-20230905113519">

<div>

  <div>
      <p> </p>
              <p><span itemprop="name">
                <a href="https://www.wsj.com/news/author/sam-schechner" itemprop="url" rel="author">Sam Schechner</a>
              </span> / </p>
            <p><span itemprop="name">
                <a href="https://www.wsj.com/news/author/deepa-seetharaman" itemprop="url" rel="author">Deepa Seetharaman</a>
              </span></p>

  </div>
    <time>
      2023年9月5日12:10 CST 更新
    </time>
</div>

<div subscriptions-actions subscriptions-display="NOT data.noSharing">
  <div>
    <social-share type="system" width="72" height="24"
      data-param-url="https://cn.wsj.com/articles/ai对人类有多大威胁-连科技领袖也莫衷一是-2e70b66e">
    </social-share>
  </div>
</div>


<div subscriptions-section="content">
</div>
<div subscriptions-section="content-not-granted">
</div>



<section subscriptions-section="content">
      <p>人工智能(AI)的先驱们正在为<a href="https://cn.wsj.com/articles/CN-TEC-20230531111020" target="_blank" >这项技术的哪种危险最可怕</a>而争论不休。</p>
      <p>由一些建造先进AI系统的高层管理人员组成的阵营认为，AI的产物可能导致巨大灾难。 而由科学家组成的另一个阵营则认为，人们应该主要关注AI现在是如何实施的，以及它在我们日常生活中会造成怎样的危害。</p>
      <p>AI开发公司Anthropic的首席执行官兼联合创始人Dario Amodei属于就人类的生死存亡危险发出警告的一派。今年夏天，他在美国国会作证时说，AI可能会给人类带来这种风险。ChatGPT的开发公司OpenAI的首席执行官Sam Altman今年春天在<a href="https://cn.wsj.com/articles/CN-BIZ-20230630110906" target="_blank" >全球巡回演讲</a>时表示，AI有一天可能会造成严重危害，甚至更糟的后果。马斯克(Elon Musk)在今年5月出席《华尔街日报》(The Wall Street Journal)举办的一次活动时说，“AI毁灭人类的可能性并非为零”。他在那不久之后创立了自己的AI公司。</p>
      <div> <p>预计Altman、马斯克等一些AI公司高管将于下周参加美国参议院多数党领袖、纽约州民主党参议员舒默(Chuck Schumer)召开的一系列AI闭门会议中的首次会议，会议将讨论包括“末日场景”在内的话题。</p>
      <p>另一个AI科学家阵营则称，这些警告是科幻小说引发的干扰信息，甚至是一种有违常规的营销策略。他们说，AI公司和监管机构应该把有限的资源集中在该技术现有的和迫在眉睫的威胁上，比如那些生成影响力很大的有关选举的<a href="https://cn.wsj.com/articles/CN-BGH-20230606113840" target="_blank" >错误信息的工具</a>，或者那些放大人类偏见影响的系统。</p>
      <p>这场争论愈演愈烈，<a href="https://cn.wsj.com/articles/CN-TEC-20230505073204" target="_blank" >世界各地的公司和政府都在试图决定将资源和注意力集中于何处</a>，以最大程度地从这项被广泛认为可能改变世界的技术中获益，并尽可能地减少不利方面。</p>
      <p>“这种分化非常现实，而且不断加剧，”Future Society联合创始人Nicolas Miailhe表示。“就好比有人觉得不过是到了月底，而有人觉得是世界末日。”Future Society是一家致力于AI治理的智库，正努力弥合这一分歧。</p>
      <p>尽管AI一直备受关注，但直到不久之前，关于AI引发的人类生存风险（最忧心忡忡的人称之为“X风险”）的严肃公开讨论仍然只局限在一小部分哲学家和AI研究人员当中。</p>
      <p>OpenAI去年年底推出ChatGPT并进行了一系列的后续改进，使其能作出类似人类的反应，从而引发了关于此类系统可能获得超人智能的警告。在那之后，上述局面发生了变化。包括被视为AI教父之一的Geoffrey Hinton在内，一些知名研究人员都认为AI技术有一点类人推理能力。Hinton今年从Alphabet (GOOG)旗下的谷歌(Google)离职，以更不受约束地讨论AI风险。</p>
      <div class="media-object scope-
        ">
          <div>
      <div>
    <div>
         <h4 itemprop="description">
        </h4>
  </div>
  </div>
      <p>
        在全美各地的快餐店，人工智能聊天机器人干起了点餐员的活。《华尔街日报》科技专栏作家Joanna Stern设计了三项挑战，在一家Hardee’s快餐店对上述技术进行了一系列测试。猜猜看，由OpenAI技术支持的点餐机器人能正确回应一些疯狂的问题，以及屏蔽狗叫声和鸣笛声等干扰吗？
        <span>
        WSJ S Chinese
        </span>
      </p>
</div>

          <!-- eventually when we know what this card will be we can change it and leave this one -->
      </div>
      <p>剑桥大学(University of Cambridge)机器学习教授David Krueger说，关于生死存亡风险的警告一直有种忌讳，人们害怕这么做会被嘲笑，被当成疯子，影响自己的职业前景。Krueger在今年5月份帮助组织了一份声明，称AI带来的灭绝风险与大流行病和核战争的危险不相上下。数以百计AI专家签署了这份声明，其中包括谷歌、OpenAI和Anthropic的高管和研究员等。</p>
      <p>Krueger说：“我想让研究人员知道，持同样看法的大有人在。”</p>
      <p>该领域的一些人认为，AI公司强调AI系统的“X风险”虽然自相矛盾但有一个好处，因为这给人一种他们的技术异常复杂的感觉。</p>
      <p>前谷歌律师Daniel Schoenberger表示：“很明显，这些人受益于仍在升温的炒作。”他表示，政策制定者应该更多地关注近期风险，比如AI使发起传播虚假和误导性信息的活动变得成本更低，或者将更多权力集中在硅谷。Schoenberger曾参与谷歌2018年AI原则清单的制定，目前在Web3 Foundation工作。</p>
      <p>Schoenberger称：“存在主导风险，即科技巨头成为AI巨头。”</p>
      <p>担心生存风险的AI领导者表示，他们的担忧是真实的，而不是一种策略。OpenAI的Altman在6月份表示：“说‘哦，政府是指望不上了，所以呼吁监管是徒劳’——这不是我们的想法。这是一种生存风险。”</p>
      <p>所谓的末日论者并没有说AI一定会像《终结者》(Terminator)电影中的“天网”那样崛起，毁灭人类。一些人担心，被训练来寻求奖励的AI系统最终可能会有隐藏的追求权力的冲动，在执行我们的愿望时无意中伤害人类，或者干脆超越人类，掌握我们的命运。这一领域的研究主要集中在所谓的一致性上，即如何确保未来的计算机头脑与我们的目标本质上是一致的。</p>
      <p>相比之下，AI伦理与公平领域的专家们担心的是这些工具如何无意或有意地剥削工人、加深数以百万计人的不平等。他们希望科技公司和监管机构执行培训标准和技术，以减少这种威胁。</p>
      <p>多样性是一个引爆点。AI伦理学家已展示了经过历史数据训练的AI系统可能如何将过去的歧视纳入未来的重要决策中，如居住、招聘或刑事判决。研究还表明，生成式AI系统可能生成带有偏见色彩的图像。他们还认为，AI研究人员缺乏多样性会导致他们看不到AI可能对有色人种和女性产生的影响。</p>
      <p>这场辩论可能会变得非常激烈。“你有什么计划来确保AI不会带来生存风险？”在今年6月举行的一个关于生存风险的公开论坛上，生命未来研究所(Future of Life Institute)所长Max Tegmark向知名AI研究员、圣达菲研究所(Santa Fe Institute)教授Melanie Mitchell发问。“你没有回答我的问题。”</p>
      <p>“我不认为存在生存风险，”Mitchell反驳道，她还称人们正努力“降低更直接的、现实世界中的风险”。Tegmark听到这番话时瞪大了眼睛。</p>
      <p>Mitchell在一次采访中说，关于生存风险的讨论“完全基于猜测，根本没有科学依据”。</p>
      <p>在麻省理工学院(Massachusetts Institute of Technology)担任教授的Tegmark表示，他认为一些公司有意在关注公平问题和生存风险的人们之间挑起分歧，以逃避监管。Tegmark创办的非营利组织未来生命研究所旨在防范技术带来极端的大规模风险。</p>
      <p>Tegmark在接受采访时表示，如果双方在这个问题上都各持己见，则将两败俱伤。</p>
      <p>多年来，这场争论已经擦出火花。2015年，在谷歌园区内举办的一次会议间隙，一些学者和科学家聚在一起讨论AI的风险。一方与担忧生存风险的人士针锋相对，认为重点应放在当下的危害上，包括各种偏见。</p>
      <p>与会的加州大学伯克利分校(University of California, Berkeley)教授Steven Weber回忆称，担忧X风险的人士当时反驳说，人类的未来岌岌可危，应该没人会担心AI可能导致房贷利率出现0.25个百分点的差异。</p>
      <p>Weber表示：“我几乎以为这场学术会议将发生一起斗殴事件。”</p>
      <p>那些担心世界末日的人士远远没有达成共识，一些末日论者认为，即使是那些表示担忧的大公司高管也没有采取足够的措施来避免世界末日。</p>
      <p>AI公司Conjecture的首席执行官Connor Leahy表示：“我们正在目睹一场荒谬的死亡竞赛，主要参与者都在争相推出神通广大的AI。”Leahy说，他对大型科技公司关于人类生死存亡风险担忧的看法持谨慎态度。他表示：“要观其行，而不是听其言。”该公司致力于解决如何让AI与人类价值观和利益保持一致的问题。</p>
      <p>人们也在努力弥合两个阵营的分歧。一些伦理学研究者表示，他们并不完全否定有关人类生死存亡的风险，只是认为应该把这作为有更明确定义的现有问题的一部分来解决。一些末日论者认为，通向灾难的道路很可能来自伦理学界所强调的问题，比如大量炮制的虚假信息导致政府被推翻或引发战争的后果。双方都希望能够揭开AI思考方式的黑箱，即所谓的可解释性问题。</p>
      <p>曾在Google DeepMind工作过的爱丁堡大学(University of Edinburgh) AI伦理学助理教授Atoosa Kasirzadeh说：“一些人确实在试图弥合这两个阵营的分歧。”Kasirzadeh表示：“希望这些群体能够相信，他们在内心深处关注的都是同一类事情。”</p>
      </div>
</section>

</div>
      </main>
